\chapter{Discussion}
 \label{chap:disc}
\section{Conclusion and Summary}

\subsection*{JIT Module: The Wrapper}

We have implemented a \emph{Wrapper} interface around the PyNEST Python module that can intercept its function calls, and depending on specific constraints, we can either apply some pre-processing and post-processing steps before calling the function or ignore it completely. The Wrapper has a straightforward structure that renders its use simple and flexible. Each targeted function in PyNEST has a custom implemented wrapper, and these wrappers are registered in a \emph{dictionary}, with the keys being the name of the targeted function and the values are the registered \emph{Wrapper} classes. Once the JIT module is imported, it iterates over the PyNEST's functions, methods, and classes, and if one of these objects is registered in the dictionary, then a wrapper instance is created and inserted in the JIT module. Non-targeted objects are inserted directly without having a wrapper in between. Another main feature of the wrappers is that they are implemented independently and do not influence each other.

\subsection*{JIT Module: The Lightweight Version}

Since the implemented wrapper for the \texttt{nest.Create} function must search for the model, and in the worst case, the code for the model must be generated, we decided to split the code generation overhead. On the call to \texttt{nest.Create}, the user will not need to wait for the whole code of the model to be generated, built, and installed. Instead, we create a \emph{lightweight} version of the model that has only the \emph{State} and \emph{Parameter} blocks, which are only the necessary attributes needed from the model after creating the instances, and then the real code generation of the models happens in the background without blocking the execution of the wrapped \texttt{nest.Create} function. Between the \texttt{nest.Create} and \texttt{nest.Connect}, the user can modify the attributes of the lightweight version by assigning new values, being either deterministic or random.


One drawback with this approach is that after calling \texttt{nest.Connect} or \texttt{nest.Simulate} the models will be instantiated for real, and then we need to copy the values of the attributes from the lightweight version to the real instances. Thus, we have the same amount of memory allocated twice, one on the \emph{Python} level and the other on the \emph{C++} level.

\subsection*{JIT Module: The NodeCollectionProxy}

In order to retrieve information from the lightweight version and to make handling transparent and similar to the \texttt{nest.NodeCollection}, we have implemented the \texttt{JitNodeCollection} and the \texttt{NodeCollectionProxy}. The relation between these three elements may be seen as a \emph{tree-like} structure. The \texttt{NodeCollectionProxy} is the root element in the \emph{tree}. It has two children, the first one is the \texttt{JitNodeCollection} and the second one is the \texttt{NodeCollection}. The \texttt{JitNodeCollection} has an arbitrary number of children. If the \emph{ids} of the children of the \texttt{JitNodeCollection} build a contiguous space, and they belong to the same model, then they are represented by the \texttt{JitNode}, which represents in the context of the tree, the \emph{leaves}. The \texttt{NodeCollectionProxy}, the \texttt{JitNodeCollection} and the \texttt{JitNode} share the same \emph{interface} that implements the support of retrieving and setting the attributes of the instances and also the support for \emph{indexing} and \emph{slicing} exactly like the provided functionalities in the \texttt{nest.NodeCollection}.

\subsection*{Vectorization}

We have introduced a new data structure in the NestKernel for handling models. The new data structure is based on the \emph{Structure of Arrays SoA}, where instead of having $n$ instances of the same model with $k$ attributes, we have exactly one instance that has $k$ attributes in the form of \emph{vectors} and each of these vectors has a size of $n$. The \emph{Vectorization} of the models behaves like a container of nodes, so instead of updating the nodes separately, we can update just the existing containers which eventually updates the states of the nodes in parallel by utilizing the efficiency of \emph{SIMD} instructions in modern processors. The \emph{Vectorization} should also decrease the amount of \emph{cache misses} due to the spatial locality of the items in the array. Consequently, we can also update large number of nodes without the need of requesting the data from the main memory multiple times.


\subsection*{Performance}

The performance of the JIT module without using an external synapse model does not deviate a lot from running the simulation script without the JIT tool. However, when using an external custom synapse, we observe a significant difference in the performance. The difference can be explained by the overhead of running some steps of the code generation pipeline twice, maintaining the functionality of the \texttt{NodeCollectionProxy} and finally replacing the network nodes and updating their parameters.

Even though the NestKernel does not fully support \emph{Vectorization} , we observe that the performance of the \emph{vectorized} model and the \emph{non-vectorized} model are quite similar on average.

\section{Outlook}

\subsection*{Modularity}

We have already raised the issue regarding the duplicate memory usage on the Python and C++ side because of the lightweight version. Another problem is in the \texttt{NodeCollectionProxy}, each change in the \texttt{nest.NodeCollection} must also be adjusted in the \texttt{NodeCollectionProxy}. Thus, maintaining the JIT project might require a bit of an overhead. To mitigate these problems, we can improve the concept of the lightweight version and make it more flexible. The solution is to adjust the code generation pipeline and make it generate and build the library in pieces. The model may be split into two pieces. The first part contains the State and Parameters blocks. These blocks will be generated and built on the call to the \texttt{nest.Create} function. The rest of the code can be then generated in the background and assembled with the first part when calling the \texttt{nest.Connect} or \texttt{nest.Simulate} function. Since the co-code generation of the neuron and synapse just moves variables from the synapse to the neuron, we may then generate the code of the synapse and split it into two parts. The first part includes building the moved variables in a separate library and then append them to the neuron State block during the simulation. The second part takes care of the rest of the functionality in the synapse and makes it available during the \texttt{nest.Connect} call.

\subsection*{Removal of the NodeCollectionProxy}

With the \emph{modularity} in play, we can completely drop the usage of the current JIT data structures (i.e., the \texttt{NodeCollectionProxy, JitNodeCollection} and \texttt{JitNode}). The NestKernel infrastructure will be adjusted to support the modularity of the models and make it possible to assemble the pieces of the model at runtime. Therefore, more sophisticated architecture of the \texttt{NodeCollection} must be designed to support modularity and make querying the model and updating their State and Parameter blocks possible even if the model is not fully built.


\subsection*{Vectorization: Updating the containers not the single nodes}

The \texttt{update} function and other important functionalities related to the simulation do not efficiently utilize the vectorized models. Each of these functions handles the instances of the different models separately and does not use the created containers that represent all instances of each model. The NestKernel must be redesigned to use the containers as the unit for the whole simulation. Instead of storing a list of instances of the \texttt{Node} class, we store instances of \texttt{VectorizedNode} as a list of containers. Each container then executes the same instructions in parallel for all the nodes it represents.

\subsection*{JIT: The Orchestrator}

After removing the \texttt{NodeCollectionProxy} with the introduced modularity, the JIT module will only have the task of assembling the pieces of the model. It will make sure that the neuron model is connected with the correct synapse type without the necessity to run the code generation pipeline again to match the synapse type with the correct neuron model.

\cleardoublepage
